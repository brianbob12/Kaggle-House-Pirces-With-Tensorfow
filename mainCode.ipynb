{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GithubVersion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "18LfwCko2NJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dependancies\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ftm7DaB6Nhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prep(row):#Formats a line of csv to an array of floats shape=[304]\n",
        "  y=row[-1]\n",
        "  row=row[:-1]\n",
        "  x=[]\n",
        "  for i in row:\n",
        "    try:\n",
        "      for j in i:\n",
        "        x.append(float(j))\n",
        "    except:\n",
        "      x.append(float(i))\n",
        "  return x,y\n",
        "\n",
        "def test(iteration):#runs a test with the hold-out set and prints the results\n",
        "  batch_x=[]        #used in order to track progress during training\n",
        "  batch_y=[]\n",
        "  for i in hold_out:\n",
        "    x,y=prep(i)\n",
        "    batch_x.append(x)\n",
        "    batch_y.append([y])\n",
        "  error=math.sqrt(sess.run(tf.reduce_mean(cross_entropy),feed_dict={X:batch_x,Y:batch_y}))\n",
        "  print(\"-\"*15)\n",
        "  print(\"iteration:\",iteration)\n",
        "  print(\"error:\",error)\n",
        "  print(\"-\"*15)\n",
        "  testing_log[0].append(iteration)#This log is used for graphs for training at the end\n",
        "  testing_log[1].append(error)\n",
        "  return error\n",
        "def liniar1(x):#this is a function for the equivelent of no funciont in the layer_activation variable\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2--aJf-Y4Zoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=pd.read_csv(\"train.csv\")\n",
        "train_data=train_data.drop(columns=[\"Id\"])\n",
        "print(\"columns:\",len(train_data.columns)-1,\" + 1(x)\")\n",
        "raw_data=train_data.copy()\n",
        "numbers=[\"MSSubClass\",\"LotArea\",\"OverallQual\",\"OverallCond\",\"YearBuilt\",\"YearRemodAdd\",\n",
        "        \"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"1stFlrSF\",\"2ndFlrSF\",\n",
        "        \"LowQualFinSF\",\"GrLivArea\",\"BsmtFullBath\",\"BsmtHalfBath\",\"BsmtHalfBath\",\"FullBath\",\n",
        "        \"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"TotRmsAbvGrd\",\"Fireplaces\",\"GarageCars\",\n",
        "        \"GarageArea\",\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\n",
        "        \"PoolArea\",\"MiscVal\",\"MoSold\",\"YrSold\",\"SalePrice\"]\n",
        "int_na=[\"GarageYrBlt\",\"LotFrontage\",\"MasVnrArea\"]\n",
        "\n",
        "for col in train_data.columns:\n",
        "  new_col=[]\n",
        "  \n",
        "  if col in numbers:\n",
        "    for i in train_data[col]:\n",
        "      new_col.append(int(i))\n",
        "  elif col in int_na:\n",
        "    for i in train_data[col]:\n",
        "      try:\n",
        "        new_col.append(int(i))\n",
        "      except:\n",
        "        new_col.append(-1)   \n",
        "  else:\n",
        "    unis=train_data[col].unique()\n",
        "    unilen=len(unis)\n",
        "    for row in train_data[col]:\n",
        "      pre_row=[]\n",
        "      for i,v in enumerate(unis):\n",
        "        if row==v:\n",
        "          pre_row.append(1)\n",
        "        else:\n",
        "          pre_row.append(0)\n",
        "      new_col.append(tuple(pre_row))\n",
        "    \n",
        "  train_data[col]=new_col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cut4YEXi8D0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SETUP HOLDOUT --OPTINAL\n",
        "#if you do not want a hold-out do not run this box\n",
        "n_hold_out=20\n",
        "hold_out=[]\n",
        "test_const=5#takes one of every 5 peices of training data for the holdout\n",
        "for i in range(n_hold_out):\n",
        "  hold_out.append(train_data.loc[i*test_const])\n",
        "  train_data=train_data.drop([i*test_const])\n",
        "\n",
        "x,y=prep(train_data.loc[1])\n",
        "print(len(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "922ZSz-SLskp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CONSTANTS\n",
        "#Constants have been optimsed\n",
        "n_input=304\n",
        "n_out=1\n",
        "n_layers=2#excludes input layer includes output layer\n",
        "layer_number=[256,1]#number of neurons in each layer, excludes input layer includes output layer\n",
        "layer_activation=[liniar1,liniar1,liniar1]#activation function for each layer\n",
        "weight_decay_value=20\n",
        "#outpus price\n",
        "\n",
        "learning_rate=0.01\n",
        "batch_size=train_data.count()[0]-n_hold_out#the whole dataset\n",
        "#redefine batch_size in order to use mini-batches\n",
        "batch_iterations=60\n",
        "total_iterations=batch_size*batch_iterations\n",
        "\n",
        "X=tf.placeholder(\"float\",[None,n_input])\n",
        "Y=tf.placeholder(\"float\",[None,n_out])\n",
        "dropout=tf.placeholder(\"float\")#if 1 then dropout occurs ,if 0 no dropout occurs\n",
        "\n",
        "test_occurance=1#in batch iterations\n",
        "\n",
        "#output the hyperperameters of the network in a clear way\n",
        "print(\"current structure:\")\n",
        "print(\"learning rate:\\t\\t\"+str(learning_rate))\n",
        "print(\"batch size:\\t\\t\"+str(batch_size))\n",
        "print(\"weight dacay value:\\t\"+str(weight_decay_value))\n",
        "print(\"\\tinput layer interger\\t-304-\")\n",
        "for i in range(n_layers-1):\n",
        "  print(\"\\thidden layer float\\t\",end=\"\")\n",
        "  print(\"-\"+str(layer_number[i])+\"-\\tactivation:\"+str(layer_activation[i]))\n",
        "print(\"\\toutput layer float\\t-1-\\tactivation:\"+str(layer_activation[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8j-6kXtz_dN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import network --OPTIONAL\n",
        "#can be sued to import a trained model\n",
        "#Make sure the constants above are applicable for the imported model\n",
        "#ONly accepts models exported from this program\n",
        "import os\n",
        "reW=[]\n",
        "reB=[]\n",
        "path=input(\"enter network path:\")\n",
        "layer_n=len(os.listdir(path+\"/weights\"))\n",
        "for i in range(layer_n):\n",
        "  reW.append([])\n",
        "  with open(path+\"/weights/l\"+str(i)+\".csv\",\"r\") as f:\n",
        "    red=f.readlines()\n",
        "  for j in red:\n",
        "    reW[i].append([float(k) for k in j[:-1].split(\",\")])\n",
        "\n",
        "with open(path+\"/biases/biases.csv\",\"r\") as f:\n",
        "  red=f.readlines()\n",
        "  reB=[[float(j) for j in i[:-1].split(\",\")] for i in red]\n",
        "\n",
        "weights=[]\n",
        "biases=[]\n",
        "layers=[]\n",
        "for i in range(n_layers):\n",
        "  biases.append(tf.Variable(reB[i]))\n",
        "  weights.append(tf.Variable(reW[i]))\n",
        "  if i==0:\n",
        "    tf.add_to_collection(\"weight_decay\",tf.nn.l2_loss(weights[-1])*weight_decay_value)\n",
        "    layers.append(layer_activation[i](tf.math.add(tf.matmul(X,weights[i]),biases[i])))\n",
        "  else:\n",
        "    layers.append(layer_activation[i](tf.math.add(tf.matmul(layers[i-1],weights[i]),biases[i])))\n",
        "out_layer=layers[-1]#take the final layer\n",
        "\n",
        "cross_entropy=tf.reduce_mean(tf.square(tf.math.divide(tf.math.subtract(out_layer,Y),Y)))\n",
        "weight_decay_loss = tf.get_collection('weight_decay')\n",
        "total_loss=cross_entropy+weight_decay_loss\n",
        "training_step=tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ISpMixIPQ0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make new networks --OPTIONAL\n",
        "#initilises a new, untrained network using the hyperperameters above\n",
        "weights=[]\n",
        "biases=[]\n",
        "layers=[]\n",
        "\n",
        "for i in range(n_layers):\n",
        "  biases.append(tf.Variable(tf.constant(0.1,shape=[layer_number[i]])))\n",
        "  if i==0:#first layer\n",
        "    weights.append(tf.Variable(#multiples random weights by a constant in order to prevent the network having too high weights at initilisation\n",
        "        tf.math.scalar_mul(tf.math.sqrt(1/layer_number[i]**(i)),tf.truncated_normal([n_input,layer_number[i]],stddev=1))\n",
        "                  ))\n",
        "    tf.add_to_collection(\"weight_decay\",tf.nn.l2_loss(weights[-1])*weight_decay_value)#sets up weight decay for L2 regulisation\n",
        "    \n",
        "    layers.append(layer_activation[i](tf.math.add(tf.matmul(X,weights[i]),biases[i])))\n",
        "    \n",
        "  else:\n",
        "    weights.append(tf.Variable(\n",
        "        tf.math.scalar_mul(tf.math.sqrt(1/layer_number[i]**(i)),tf.truncated_normal([layer_number[i-1],layer_number[i]],stddev=1))\n",
        "                  ))\n",
        "    layers.append(layer_activation[i](tf.math.add(tf.matmul(layers[i-1],weights[i]),biases[i])))\n",
        "\n",
        "out_layer=layers[-1]#take the final layer\n",
        "\n",
        "cross_entropy=tf.reduce_mean(tf.square(tf.math.divide(tf.math.subtract(out_layer,Y),Y)))#gets the mean of the square of the difference between prediction and correct answer\n",
        "weight_decay_loss = tf.get_collection('weight_decay')\n",
        "total_loss=cross_entropy+weight_decay_loss#weight_decal_loss is part of L2 regularization\n",
        "training_step=tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcgf39_JOMoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_log=[[],[],[]]\n",
        "init=tf.global_variables_initializer()\n",
        "sess=tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ19TUY_P2MU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train the network\n",
        "last=test(0)\n",
        "place=0#excludes test data\n",
        "for i in range(batch_iterations):\n",
        "  batch_x=[]\n",
        "  batch_y=[]\n",
        "  for j in range(batch_size):\n",
        "    while True:\n",
        "      try:\n",
        "        x,y=prep(train_data.loc[place])\n",
        "      except:\n",
        "        place+=1\n",
        "        place%=train_data.count()[0]#loop trought the dataset\n",
        "      finally:\n",
        "        break\n",
        "    place+=1\n",
        "    place%=train_data.count()[0]\n",
        "    batch_x.append(x)\n",
        "    batch_y.append([y])\n",
        "  results=sess.run([cross_entropy,training_step],feed_dict={X:batch_x,Y:batch_y})\n",
        "  if (i+1)%test_occurance==0:\n",
        "    k=math.sqrt(results[0])\n",
        "    print(\"training error:\",k)\n",
        "    testing_log[2].append(k)\n",
        "    p=test(i+1)\n",
        "    if p-last>0:\n",
        "      mod=0\n",
        "      print(\"minimum hold-out:\"+str(min(testing_log[1])))\n",
        "      print(\"final hold-out:  \"+str(testing_log[1][-1]))\n",
        "      plt.plot(testing_log[0][mod:],testing_log[1][mod:],\"r\")#graphing error\n",
        "      plt.plot(testing_log[0][mod:-1],testing_log[2][mod:],\"b\")#graphing training error\n",
        "      plt.show()\n",
        "      if input(\"Negitive progres, stop learning(y/n)\").lower() == \"y\":\n",
        "        break\n",
        "      else:\n",
        "        last=p\n",
        "    else:\n",
        "      last=p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A5l0-fYWfxh",
        "colab_type": "code",
        "outputId": "4bace4b9-a952-41ea-8713-453c3ccd67bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "#Graph the training\n",
        "#red is training error\n",
        "#blue is hold-out error\n",
        "mod=0\n",
        "print(\"minimum hold-out:\"+str(min(testing_log[1])))\n",
        "print(\"final hold-out:  \"+str(testing_log[1][-1]))\n",
        "plt.plot(testing_log[0][mod:],testing_log[1][mod:],\"r\")#graphing error\n",
        "plt.plot(testing_log[0][mod:-1],testing_log[2][mod:],\"b\")#graphing training error\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minimum hold-out:0.4817691974783953\n",
            "final hold-out:  0.48351294192101174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ/VkgAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdFLbvzIOXY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test of the network on individual peices of the training data\n",
        "print(\"number from 0-1459(inclusinve)\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "num=int(input(\"number:\"))\n",
        "if num>1459:\n",
        "  print(\"too large\")\n",
        "elif num<0:\n",
        "  print(\"too large\")\n",
        "else:\n",
        "  try:\n",
        "    test_row=train_data.loc[num]\n",
        "  except:\n",
        "    print(\"plesase chose a different number\")\n",
        "  else:\n",
        "    if num%5==0:\n",
        "      print(\"The network was not trained on this peice of data.\")\n",
        "    else:\n",
        "      print(\"The network was trained on this peice of data.\")\n",
        "    #print(raw_data.loc[num:num])\n",
        "    x,y=prep(test_row)\n",
        "    outs=sess.run([cross_entropy,out_layer],feed_dict={X:[x],Y:[[y]]})\n",
        "    print()\n",
        "    print(\"Predicted value:\\t $\"+str(outs[1][0][0]))\n",
        "    print(\"Actual value:\\t\\t $\"+str(y))\n",
        "    print(\"difference:\\t\\t $\"+str(outs[1][0][0]-y))\n",
        "    print(\"error:     \\t\\t\"+str(abs(outs[1][0][0]-y)/y*100)+\"%\")\n",
        "raw_data.loc[num:num]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq4nC9-NJrrF",
        "colab_type": "code",
        "outputId": "9fd2d9bd-04f6-4895-eb03-e9756c78c852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Export Network\n",
        "final_weights=sess.run(weights)\n",
        "final_baises=sess.run(biases)\n",
        "import os\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"network\")\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  os.mkdir(\"network//weights\")\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  os.mkdir(\"network//biases\")\n",
        "except:\n",
        "  pass\n",
        "for n,v in enumerate(final_weights):\n",
        "  name=\"network//weights//l\"+str(n)+\".csv\"\n",
        "  final=\"\"\n",
        "  for i in v:\n",
        "    for j in i:\n",
        "      final+=str(j)+\",\"\n",
        "    final=final[:-1]\n",
        "    final+=\"\\n\"\n",
        "  with open(name,\"w\") as f:\n",
        "    f.write(final)\n",
        "fin=\"\"\n",
        "for i in final_baises:\n",
        "  for j in i:\n",
        "    fin+=str(j)+\",\"\n",
        "  fin=fin[:-1]\n",
        "  fin+=\"\\n\"\n",
        "with open(\"network//biases//biases.csv\",\"w\") as f:\n",
        "  f.write(fin)\n",
        "print(\"Network Exported\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network Exported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEDy74Q1ShvY",
        "colab_type": "code",
        "outputId": "8d84345f-3209-4c59-e702-b5e7926a5ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#DOwnlaods the exported network for Google Colab\n",
        "!zip -r /content/network.zip /content/network\n",
        "from google.colab import files\n",
        "files.download(\"/content/network.zip\") \n",
        "print(\"network Downloaded\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/network/ (stored 0%)\n",
            "  adding: content/network/biases/ (stored 0%)\n",
            "  adding: content/network/biases/biases.csv (deflated 62%)\n",
            "  adding: content/network/weights/ (stored 0%)\n",
            "  adding: content/network/weights/l1.csv (deflated 54%)\n",
            "  adding: content/network/weights/l0.csv (deflated 57%)\n",
            "network Downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}